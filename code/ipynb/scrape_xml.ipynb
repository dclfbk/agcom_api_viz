{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "import os\n",
    "import chardet \n",
    "from spellchecker import SpellChecker\n",
    "from langdetect import detect\n",
    "import string\n",
    "import wikipediaapi\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='it')\n",
    "special_words = {\"TG5\", \"TG24\",\"MAG\",\"OMNIBUS\",\"LA7\",\"UOZZAP!\",\n",
    "                 \"TAGADA'\",\"DIMARTEDI'\",\"MARTEDIPIU'\",\"MEDIASET\",\n",
    "                 \"TGCOM\",\"TGCOM24\",\"TG4\",'CONTROCORRENTE',\"DEEJAY\",\n",
    "                 \"CROZZA\",\"RAI\",\"TG1\",\"SETTEGIORNI\",\"(L.I.S.)\",\n",
    "                 \"150ESIMO\",\"1/2H\",\"#CARTABIANCA\",\"NEWS24\",\n",
    "                 \"PARLAMENTANDO\",\"SABATO24\",\"CONTROCORRENTE\",\n",
    "                 \"(VN)\",\"PRES.\",\"CATTELAN\",\"PIAZZAPULITA\",\"SILVIO\",\n",
    "                 \"SPAZIOLIBERO\",\"TG3\",\"DIGITANGO\",\"BERLUSCONI\",\"COSTANZO\",\n",
    "                 \"SIENA\",\"CROZZA\",\"MARMOLADA\",\"LA7\",\n",
    "                 \"COVID-19\", \"OK\", \"CEO\", \"NASA\", \"Wi-Fi\"}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Endpoint URL di Wikidata per le query SPARQL\n",
    "# endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# # Query SPARQL per ottenere i nomi dei politici italiani\n",
    "# query_politicians = \"\"\"\n",
    "# SELECT DISTINCT ?givenNameLabel ?familyNameLabel WHERE {\n",
    "#   ?person wdt:P31 wd:Q5;           # Instance of human\n",
    "#           wdt:P106 wd:Q82955;      # Occupation: politician\n",
    "#           wdt:P27 wd:Q38;          # Country of citizenship: Italy\n",
    "#           wdt:P735 ?givenName;     # Given name\n",
    "#           wdt:P734 ?familyName.    # Family name\n",
    "#   SERVICE wikibase:label {         # Get the labels\n",
    "#     bd:serviceParam wikibase:language \"it\".\n",
    "#     ?givenName rdfs:label ?givenNameLabel.\n",
    "#     ?familyName rdfs:label ?familyNameLabel.\n",
    "#   }\n",
    "# }\n",
    "# ORDER BY ?givenNameLabel ?familyNameLabel\n",
    "# \"\"\"\n",
    "# query_movements = \"\"\"\n",
    "# SELECT ?politicalMovement ?politicalMovementLabel WHERE {\n",
    "#   ?politicalMovement wdt:P31 wd:Q7278;  # Istanza di organizzazione politica\n",
    "#                  wdt:P17 wd:Q38.        # Localizzata in Italia\n",
    "#   SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],it\". }\n",
    "# }\n",
    "# \"\"\"\n",
    "# def get_results(endpoint_url, query):\n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'}\n",
    "#     data = requests.get(endpoint_url, headers=headers, params={'query': query, 'format': 'json'}).json()\n",
    "#     return data\n",
    "\n",
    "# politicians_data = get_results(endpoint_url, query_politicians)\n",
    "# politicians_movements_data = get_results(endpoint_url, query_movements)\n",
    "# politicians = {}\n",
    "# politicians_movements = {}\n",
    "# politicians_names = [(result['givenNameLabel']['value']) for result in politicians_data['results']['bindings']]\n",
    "# politicians_familynames = [(result['familyNameLabel']['value']) for result in politicians_data['results']['bindings']]\n",
    "# politicians_names = list(set(politicians_names))\n",
    "# politicians_familynames = list(set(politicians_familynames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_accent(sentence):\n",
    "    if sentence is not None:\n",
    "        substituions = {\n",
    "            \"E'\": \"È\",\n",
    "            \"e'\": \"è\",\n",
    "            \"A'\": \"À\",\n",
    "            \"a'\": \"à\",\n",
    "            \"I'\": \"Ì\",\n",
    "            \"i'\": \"ì\",\n",
    "            \"O'\": \"Ò\",\n",
    "            \"o'\": \"ò\",\n",
    "            \"U'\": \"Ù\",\n",
    "            \"u'\": \"ù\"\n",
    "        }\n",
    "        \n",
    "        for k, v in substituions.items():\n",
    "            sentence = re.sub(re.escape(k), v, sentence)\n",
    "        \n",
    "    return sentence\n",
    "    \n",
    "\n",
    "\n",
    "# Function to check for unrecognized characters\n",
    "def needs_spellcheck(s):\n",
    "    need = False\n",
    "    # Definition of the alphabet and accepted characters\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    numbers = \"0123456789\"\n",
    "    punctuation_characters = \".,:;!?-'\\\"()[]{}\\\\\"\n",
    "    special_characters = \"#&@\"\n",
    "    math_characters = \"+-*/=%^\"\n",
    "    accents=\"àèìòùáéíóúäëïöü\"\n",
    "    # Create the pattern that includes all accepted characters\n",
    "    accepted_characters = alphabet + numbers + punctuation_characters + special_characters + math_characters + accents+ \" \"\n",
    "    # Create the regex pattern to find unaccepted characters\n",
    "    pattern = f\"[^{re.escape(accepted_characters)}]\"\n",
    "    if re.search(pattern, s) is not None:\n",
    "        need = True\n",
    "    return need\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_politician_movement_name(name):\n",
    "    if name is not None:\n",
    "        best_match = process.extractOne(name,list(politicians_movements.values()))\n",
    "        limit = 90  \n",
    "        if best_match[1] >= limit:\n",
    "            return best_match[0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_title(text):\n",
    "    if text is not None:\n",
    "        conjunctions = {\"e\", \"ma\", \"o\", \"né\", \"che\", \"se\", \"per\", \"con\", \"di\", \"a\", \"da\", \"in\", \"su\", \"tra\", \"fra\"}\n",
    "        words = text.split()    \n",
    "        capitalized_words = [\n",
    "            word.capitalize() if word.lower() not in conjunctions else word.lower()\n",
    "            for word in words\n",
    "        ]\n",
    "        return ' '.join(capitalized_words)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, outputfile,encoding=None):\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url,headers=headers)\n",
    "    if encoding is not None:\n",
    "            response.encoding = 'utf-8-sig'\n",
    "    with open(outputfile, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path): \n",
    "    with open(file_path, 'rb') as file: \n",
    "        detector = chardet.universaldetector.UniversalDetector() \n",
    "        for line in file: \n",
    "            detector.feed(line) \n",
    "            if detector.done: \n",
    "                break\n",
    "        detector.close() \n",
    "    return detector.result['encoding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page to scrape\n",
    "rooturl = \"https://www.agcom.it/\"\n",
    "url_desc = \"taxonomy/term/277\"\n",
    "url = rooturl + url_desc\n",
    "# Custom headers with a User-Agent\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  # Check that the request was successful\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = 0\n",
    "# Find the pagination element\n",
    "pagination = soup.find('ul', class_='pagination pager__items js-pager__items')\n",
    "# Extract the aria-label attribute\n",
    "if pagination:\n",
    "    aria_label = pagination.get('aria-label', '')\n",
    "    # Extract the value after \"of\"\n",
    "    if \"of\" in aria_label:\n",
    "        total_pages = int(aria_label.split(\"of\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for pageindex in range(total_pages):\n",
    "    pageurl = url + \"?page=\" + str(pageindex)\n",
    "    response = requests.get(pageurl, headers=headers)\n",
    "    response.raise_for_status()  # Check that the request was successful\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    read_more_links = soup.find_all('a', class_='read-more')\n",
    "    # Extract the href attributes\n",
    "    hrefs.extend([link.get('href') for link in read_more_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataxml = []\n",
    "for href in hrefs:\n",
    "    dataurl = rooturl + href.lstrip('/')\n",
    "    response = requests.get(dataurl, headers=headers)\n",
    "    response.raise_for_status()  # Check that the request was successful\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    xml_links = soup.find_all('a', type='application/xml')\n",
    "    dataxml.extend([link.get('href') for link in xml_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfile = dataxml[0]\n",
    "dataurl = rooturl + xmlfile.lstrip('/')\n",
    "filename = os.path.basename(dataurl)\n",
    "download_file(dataurl,filename)\n",
    "encoding = detect_encoding(filename).lower()\n",
    "xml_content = \"\"\n",
    "with open(filename, 'r', encoding=encoding) as file:\n",
    "    xml_content = file.readlines()[1:]  \n",
    "    xml_content = ''.join(xml_content)  \n",
    "data_toclean = pd.read_xml(StringIO(xml_content), parser='lxml')\n",
    "data_toclean = data_toclean.iloc[2:, :]\n",
    "del data_toclean['TESTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xmlfile in dataxml[1:]:\n",
    "    dataurl = rooturl + xmlfile.lstrip('/')\n",
    "    filename = os.path.basename(dataurl)\n",
    "    download_file(dataurl,filename)\n",
    "    encoding = detect_encoding(filename).lower()\n",
    "    xml_content = \"\"\n",
    "    with open(filename, 'r', encoding=encoding) as file:\n",
    "        xml_content = file.readlines()[1:]  # Legge tutte le righe eccetto la prima\n",
    "        xml_content = ''.join(xml_content)  # Unisce le righe in una singola stringa\n",
    "    \n",
    "    df= pd.read_xml(StringIO(xml_content), parser='lxml')\n",
    "    df = df.iloc[2:, :]\n",
    "    if 'TESTO' in df.columns:\n",
    "        del df['TESTO']\n",
    "    data_toclean = pd.concat([data_toclean, df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['ARGOMENTO','PROGRAMMA','MICRO_CATEGORIA'] #,'COGNOME','NOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_question_marks(s):\n",
    "    if s is not None:\n",
    "        return re.sub(r'(?<! )\\?(?! )', '-', s)\n",
    "\n",
    "def correct_s(s):\n",
    "    single = True\n",
    "    if s is not None:\n",
    "        if needs_spellcheck(s):\n",
    "            words = s.split()\n",
    "            for idx in range(len(words)):\n",
    "                if needs_spellcheck(words[idx]):\n",
    "                    if len(words[idx]) > 1:\n",
    "                        cword = spell.correction(words[idx])\n",
    "                        if cword is not None:\n",
    "                            words[idx] = cword\n",
    "                        else:\n",
    "                            if words[idx][:-1].isdigit():\n",
    "                                cword =  words[idx][:-1] + \"°\"\n",
    "                                words[idx] = cword\n",
    "                        single = False\n",
    "            if single:\n",
    "                s = spell.correction(s)\n",
    "                sigle = False\n",
    "            else:\n",
    "                s = ' '.join(words)\n",
    "        s = correct_accent(s)\n",
    "        s = replace_question_marks(s)\n",
    "    return s\n",
    "        \n",
    "def get_correctvalue(v,dictionary):\n",
    "    print(dictionary[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in fields:\n",
    "    new_labels = {}\n",
    "    labels = data_toclean[field].unique()\n",
    "    for label in labels:\n",
    "        new_labels[label] = correct_s(label)\n",
    "    data_toclean[field] = data_toclean[field].map(new_labels).fillna(data_toclean[field])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_toclean.drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.DURATA = data.DURATA.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertime(datestring):\n",
    "    rdatestring = None\n",
    "    if \".\" in datestring:\n",
    "        rdatestring = pd.to_datetime(datestring,  format=\"%d.%m.%Y\")\n",
    "    elif \"/\" in datestring:\n",
    "        rdatestring = pd.to_datetime(datestring,  format=\"%d/%m/%Y\")\n",
    "    return(rdatestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DATA'] = data['DATA'].apply(lambda x: x.replace(\" 00:00\", \"\"))\n",
    "data['DATA'] = data['DATA'].apply(lambda x: x.replace(\":00\", \"\"))\n",
    "data['DATA'] = data.DATA.apply(convertime) # = pd.to_datetime(data[\"DATA\"],  format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {\"CANALE\":\"channel\",\n",
    "        \"PROGRAMMA\":\"program\",\n",
    "        \"DATA\":\"day\",\n",
    "        \"COGNOME\":\"lastname\",\n",
    "        \"NOME\":\"name\",\n",
    "        \"MICRO_CATEGORIA\":\"subcategory\",\n",
    "        \"ARGOMENTO\":\"topic\",\n",
    "        \"DURATA\":\"duration\",\n",
    "        \"TIPO_TEMPO\":\"kind\"}\n",
    "data.rename(columns=cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_politician_name(name):\n",
    "    politicians_names = [\"Niccolò Maria\",\"Nicolò\",\"Niccolò\",\"Giosuè\",\n",
    "                         \"Totò\",\"Desirè\",\"Andrè\",\"Cécile\",\"Giovì\",'Jose',\"Alì Listi\"]\n",
    "    if name is not None:\n",
    "        best_match = process.extractOne(name,politicians_names)\n",
    "        limit = 90  \n",
    "        if best_match[1] >= limit:\n",
    "            return best_match[0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_politician_lastname(name):\n",
    "    politicians_names = [\"Bonafè\",\"Cassì\",\"Schininà\",\"Patanè\",\n",
    "                         \"Santanchè\",\"Mulé\",\"Carrà\",\"Aricò\",\n",
    "                         \"Verzè\",'Scutellà',\"Cantù\",\"Zappalà\",\n",
    "                         \"Germanà\",\n",
    "                         \"D'Alì\" #ciro\n",
    "                         ,\"D'Alò\" #antonio\n",
    "                         ,\"Miccichè\",\"Lucà\",\"Raschillà\",\n",
    "                         \"Saccà\",\"Urzì\",\"Cirinnà\",\"Falcomatà\",\n",
    "                         \"Danzì\",\"Bertolè\",\"Porrà\",\"Calabrò\",\n",
    "                         \"Macrì\",\"Salmè\",\"Durì\",\"Schilirò\",\n",
    "                         \"Niceta Potì\",'Scatà',\"Alverà\",\"Scarfò\"\n",
    "                         \"D'Incà\",\"Samonà\",\"Dessì\",\"Sperlì\",\n",
    "                         \"Chiricò\",\"Andò\",\"Muscarà\",\"Chissalè\",\n",
    "                         \"Pettenà\",\"Zuccalà\",\"Carlà Campa\",\"Currò\",\"Calò\",\n",
    "                         \"Daccò\",\"Montà\",\"Gioffrè\",\"Granà\",\"Ciagà\",\n",
    "                         \"Mazzà\",\"Alì\",\"Palù\",\"Carlà\",\"Figà Talamanca\",\n",
    "                         \"Mammì\",\"Praticò\",'Pellicanò',\"Cannistrà\",\n",
    "                         \"Verì\",\"Orrù\",\"Borrè\",'Wagué',\"Scottà\",\"Virzì\",\n",
    "                         \"Idà\",\"Pavorè\",\"Spanò\",'Venè',\"Cà\",'Cuè',\n",
    "                         \"Crisà\",\"Nicolò\",\"Niccolò\",\"Giuffrè\",\n",
    "                         \"Benvegnù\",\"Marcianò\",\"Dehò\",\"Scamardì\",\n",
    "                         \"Schirò\",\"Rösch\",\"Calà Lesina\",\"Gullà\",\n",
    "                         \"Faccà\",\"Marù\",\"D'Angeli\",\"D'Alberto\",\n",
    "                         \"Ballarò\",\"Chittò\",\"Maffè\",\"Serranò\",\n",
    "                         \"Verità\",\"Ingrillì\",\"Giampà\",\"Spirlì\"\n",
    "\n",
    "                         ]\n",
    "    if name is not None:\n",
    "        best_match = process.extractOne(name,politicians_names)\n",
    "        limit = 80  \n",
    "        if best_match[1] >= limit:\n",
    "            return best_match[0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lastnames = {}\n",
    "movementstofix = {}\n",
    "movementstofix[\"Italia Viva - Italia C'�\"] = \"Italia Viva - Italia C'è\"\n",
    "movementstofix[\"L'Italia s'� desta\"] = \"L'Italia s'è desta\"\n",
    "movementstofix[\"Movimento Verde � Popolare\"] = \"Movimento Verde - Popolare\"\n",
    "movementstofix[\"Comitato io Dico S�\"] = \"Comitato io Dico Sì\"\n",
    "movementstofix[\"LAlternativa c�-Lista del Popolo per la Costituzione\"] = \"LAlternativa c'è-Lista del Popolo per la Costituzione\"\n",
    "movementstofix[\"Azione - +Europa � Radicali Italiani\"] = \"Azione - +Europa - Radicali Italiani\"\n",
    "movementstofix[\"Facciamo Eco � Federazione dei Verdi\"] = \"Facciamo Eco - Federazione dei Verdi\"\n",
    "movementstofix[\"Movimento Nazionale per la Sovranit�\"] = \"Movimento Nazionale per la Sovranità\"\n",
    "movementstofix[\"Pi� Europa-Centro Democratico\"] = \"Più Europa-Centro Democratico\"\n",
    "movementstofix[\"Democrazia � Libert� - La Margherita\"] = \"Democrazia - Libertà - La Margherita\"\n",
    "movementstofix[\"Chiamparino per il Piemonte del s�\"] = \"Chiamparino per il Piemonte del sì\"\n",
    "movementstofix[\"Pi� Europa\"] = \"Più Europa\"\n",
    "movementstofix[\"L'Italia � popolare\"] = \"L'Italia è popolare\"\n",
    "movementstofix[\"Ragione e Libert�\"] = \"Ragione e Libertà\"\n",
    "movementstofix[\"Il s� delle Libert�\"] = \"Il sì delle Libertà\"\n",
    "movementstofix[\"S� Toscana a Sinistra\"] = \"Sì Toscana a Sinistra\"\n",
    "movementstofix[\"Comitato Democratici per il S�\"] = \"Comitato Democratici per il Sì\"\n",
    "movementstofix[\"Centro Democratico � Italiani in Europa\"] = \"Centro Democratico - Italiani in Europa\"\n",
    "for name in data.lastname.unique():\n",
    "    if needs_spellcheck(name):\n",
    "        nm = correct_politician_lastname(name)\n",
    "        if nm != name:\n",
    "            new_lastnames[name] = nm\n",
    "        else:\n",
    "            new_lastnames[name] = movementstofix[name]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lastname\"] = data[\"lastname\"].map(new_lastnames).fillna(data[\"lastname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['name'] == 'Antonio') & (data['lastname'] == \"D'Alì\"), 'name'] = \"D'Alò\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"..\" + os.sep + \"docs\"  + os.sep +  \"data\" + os.sep + \"agcomdata.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
